# Databricks notebook source
# MAGIC %md
# MAGIC ### Define schema explicitly

# COMMAND ----------

from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType

crop_production_schema = StructType([
  StructField("State", StringType(), True),
  StructField("District", StringType(), True),
  StructField("Crop", StringType(), True),
  StructField("Year", StringType(), True),
  StructField("Season", StringType(), True),
  StructField("Area", DoubleType(), True),
  StructField("Area Units", StringType(), True),
  StructField("Production", DoubleType(), True),
  StructField("Production Units", StringType(), True),
  StructField("Yield", DoubleType(), True)
])

# COMMAND ----------

raw_path = (
    "/Volumes/workspace/capstone_project/raw/"
    "India Agriculture Crop Production.csv"
)

df_raw = (
    spark.read
         .option("header", "true")
         .schema(crop_production_schema)
         .csv(raw_path)
)

display(df_raw)
df_raw.printSchema()


# COMMAND ----------

# MAGIC %md
# MAGIC ### Column name standardization

# COMMAND ----------

df_bronze = df_raw

for c in df_bronze.columns:
    df_bronze = df_bronze.withColumnRenamed(
        c,
        c.strip().lower().replace(" ","_")
    )

df_bronze.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Bronze output path

# COMMAND ----------

bronze_path = "/Volumes/workspace/capstone_project/bronze/crop_production"


# COMMAND ----------

# MAGIC %md
# MAGIC ### Write as Parquet

# COMMAND ----------

(
    df_bronze.write
    .mode("overwrite")
    .format("parquet")
    .save(bronze_path)
)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Create Bronze table over the files

# COMMAND ----------

(
    df_bronze.write
    .mode("overwrite")
    .format("delta")
    .saveAsTable("workspace.capstone_project.bronze_crop_production")
)
