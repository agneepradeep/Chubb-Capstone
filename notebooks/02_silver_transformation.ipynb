# Databricks notebook source
df_bronze = spark.table("workspace.capstone_project.bronze_crop_production")

display(df_bronze)
df_bronze.printSchema()


# COMMAND ----------

# MAGIC %md
# MAGIC ### Handle invalid numeric values, NULLs & blanks

# COMMAND ----------

from pyspark.sql.functions import col, trim, length

df_clean = df_bronze.filter(
    (col("crop").isNotNull()) &
    (length(trim(col("crop"))) > 0) &
    (col("area").isNotNull()) &
    (col("area") > 0) &
    (col("production").isNotNull()) &
    (col("production") >= 0) &
    (col("yield").isNotNull()) &
    (col("yield") >= 0)
)



# COMMAND ----------

# MAGIC %md
# MAGIC ### Parse Financial Year (2001-02) correctly

# COMMAND ----------

from pyspark.sql.functions import split, col

df_clean = (
    df_clean
    .withColumn("year_start", split(col("year"), "-").getItem(0).cast("int"))
    .withColumn("year_end", (split(col("year"), "-").getItem(0).cast("int") + 1))
)

df_clean = df_clean.withColumnRenamed("year", "year_label")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Standardize text fields

# COMMAND ----------

from pyspark.sql.functions import initcap

string_cols = ["state", "district", "crop", "season"]

for c in string_cols:
    df_clean = df_clean.withColumn(c, initcap(trim(col(c))))


# COMMAND ----------

# MAGIC %md
# MAGIC ### Recalculating Yield & Decimal Precision

# COMMAND ----------

from pyspark.sql.functions import round

df_clean = df_clean.withColumn(
    "yield",
    round(col("production") / col("area"), 4)
)


# COMMAND ----------

# MAGIC %md
# MAGIC ### Save Transformation Progress

# COMMAND ----------

df_clean.write \
    .mode("overwrite") \
    .format("delta") \
    .option("mergeSchema", "true") \
    .saveAsTable("workspace.capstone_project.silver_crop_production")

# COMMAND ----------

display(df_clean)