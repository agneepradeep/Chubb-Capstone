# Databricks notebook source
# MAGIC %md
# MAGIC ### Read Silver table

# COMMAND ----------

df_silver = spark.table("workspace.capstone_project.silver_crop_production")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Dimension: dim_crop

# COMMAND ----------

from pyspark.sql.functions import col, monotonically_increasing_id

dim_crop = (
    df_silver
    .select("crop")
    .distinct()
    .withColumnRenamed("crop", "crop_name")
    .withColumn("crop_id", monotonically_increasing_id())
    .withColumn("crop_id",col("crop_id").cast("int"))
    .select("crop_id", "crop_name")
)

display(dim_crop)
dim_crop.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Save dimension

# COMMAND ----------

(
    dim_crop.write
    .mode("overwrite")
    .format("delta")
    .saveAsTable("workspace.capstone_project.dim_crop")
)


# COMMAND ----------

# MAGIC %md
# MAGIC ### Dimension: dim_region

# COMMAND ----------

dim_region = (
    df_silver
    .select("state", "district")
    .distinct()
    .withColumn("region_id", monotonically_increasing_id())
    .withColumn("region_id", col("region_id").cast("int"))
    .select("region_id", "state", "district")
)

display(dim_region)
dim_region.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Save dimension

# COMMAND ----------

(
    dim_region.write
    .mode("overwrite")
    .format("delta")
    .saveAsTable("workspace.capstone_project.dim_region")
)


# COMMAND ----------

# MAGIC %md
# MAGIC ### Dimension: dim_season

# COMMAND ----------

dim_season = (
    df_silver
    .select("season")
    .distinct()
    .withColumn("season_id", monotonically_increasing_id())
    .withColumn("season_id", col("season_id").cast("int"))
    .select("season_id", "season")
)

display(dim_season)
dim_season.printSchema()


# COMMAND ----------

# MAGIC %md
# MAGIC ### Save dimension

# COMMAND ----------

(
    dim_season.write
    .mode("overwrite")
    .format("delta")
    .saveAsTable("workspace.capstone_project.dim_season")
)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Dimension: dim_time

# COMMAND ----------

from pyspark.sql.functions import concat, lit

dim_time = (
    df_silver
    .select("year_start", "year_end", "year_label")
    .distinct()
    .withColumn("time_id", monotonically_increasing_id())
    .withColumn("time_id", col("time_id").cast("int"))
    .select("time_id", "year_start", "year_end", "year_label")
)

display(dim_time)
dim_time.printSchema()


# COMMAND ----------

# MAGIC %md
# MAGIC ### Save Dimension

# COMMAND ----------

(
    dim_time.write
    .mode("overwrite")
    .format("delta")
    .saveAsTable("workspace.capstone_project.dim_time")
)